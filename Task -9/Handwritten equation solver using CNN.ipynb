{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdc7e2e",
   "metadata": {},
   "source": [
    "# LetsGrowMore (LGMVIP) - \"DATA SCIENCE INTERNSHIP\"\n",
    "\n",
    "## LGMVIP July-22\n",
    "\n",
    "### _Author - Zecil Jain_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8e0a7",
   "metadata": {},
   "source": [
    "### Task - 9 Handwritten Equation solver using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd048f",
   "metadata": {},
   "source": [
    "This project is aimed at developing a mathematical equation solver using character and symbol recognition by enforcing the image processing techniques and CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c52e53",
   "metadata": {},
   "source": [
    "- The dataset is huge and very comprehensive. Since we don't have the adequate provisions in terms of high-end laptop for extracting multitudes of images and analyzing them, we will just make use of a fraction of dataset including the numbers and '+','-' and '*' operators to form a basic equation and test our model in realtime.\n",
    "\n",
    "\n",
    "- So all in all, the equation can contain any digit from 0-9 and symbol +,x,- Works on image with white background and digits/symbols are in black.\n",
    "\n",
    "\n",
    "- Future scope of this model can include analyzing all forms of equations by including the complete dataset and including every single operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24227b7",
   "metadata": {},
   "source": [
    "Dataset link: https://www.kaggle.com/xainano/handwrittenmathsymbols "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b4cc2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25829c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "np.random.seed(1212)\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6aa58a",
   "metadata": {},
   "source": [
    "Now that we have imported all our libraries, we will move onto the next step which is to load the image sets from our local drive onto the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037059f",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac53aa",
   "metadata": {},
   "source": [
    "We will be creating a load_images_from_folder function for loading image sets. The reason behind creating a function is because there are a multitude of images in a number of folders in and we have to load all these images from seperate folders onto our notebook. To not repeat typing the same block of code over and over, we will just create a function and run that numerous times till we have loaded all the required images sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd59376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    train_data = [] # empty list for storing the image sets\n",
    "    for filename in os.listdir(folder):\n",
    "        # Reading the images one by one using CV2 library\n",
    "        img = cv2.imread(os.path.join(folder, filename), cv2.IMREAD_GRAYSCALE)\n",
    "        img =~ img\n",
    "        if img is not None:\n",
    "            ret, thresh = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "            ctrs, hie = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            cnt = sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "            w = int(28)\n",
    "            h = int(28)\n",
    "            max_i = 0\n",
    "            for c in cnt:\n",
    "                x,y,w,h = cv2.boundingRect(c)\n",
    "                max_i = max(w*h,max_i)\n",
    "                if max_i==(w*h):\n",
    "                    max_x = x\n",
    "                    max_y = y\n",
    "                    max_w = w\n",
    "                    max_h = h\n",
    "            img_crop = thresh[max_y : max_y + max_h + 10, max_x : max_x + max_w + 10]\n",
    "            img_resize = cv2.resize(img_crop, (28,28))\n",
    "            img_resize = np.reshape(img_resize, (784, 1))\n",
    "            train_data.append(img_resize)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff306aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4426cd",
   "metadata": {},
   "source": [
    "Now we will start loading the various image sets from different folders using our function and store them in seperate columns in the above defined empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f510248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33997\n"
     ]
    }
   ],
   "source": [
    "# Assigning '-' to column 10\n",
    "data = load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/-')\n",
    "len(data)\n",
    "for i in range(0, len(data)):\n",
    "    data[i] = np.append(data[i], ['10'])\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6991be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59109\n"
     ]
    }
   ],
   "source": [
    "# Assigning '+' to column 11\n",
    "data11=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/+')\n",
    "\n",
    "for i in range(0,len(data11)):\n",
    "    data11[i]=np.append(data11[i],['11'])\n",
    "data=np.concatenate((data,data11))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63dc2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66023\n"
     ]
    }
   ],
   "source": [
    "# Assigning '0' to column 0\n",
    "data0=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/0')\n",
    "\n",
    "for i in range(0,len(data0)):\n",
    "    data0[i]=np.append(data0[i],['0'])\n",
    "data=np.concatenate((data,data0))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a15295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92543\n"
     ]
    }
   ],
   "source": [
    "# Assigning '1' to column 1\n",
    "data1=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/1')\n",
    "\n",
    "for i in range(0,len(data1)):\n",
    "    data1[i]=np.append(data1[i],['1'])\n",
    "data=np.concatenate((data,data1))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e7657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118684\n"
     ]
    }
   ],
   "source": [
    "# Assigning '2' to column 2\n",
    "data2=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/2')\n",
    "\n",
    "for i in range(0,len(data2)):\n",
    "    data2[i]=np.append(data2[i],['2'])\n",
    "data=np.concatenate((data,data2))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "130aec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129593\n"
     ]
    }
   ],
   "source": [
    "# Assigning '3' to column 3\n",
    "data3=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/3')\n",
    "\n",
    "for i in range(0,len(data3)):\n",
    "    data3[i]=np.append(data3[i],['3'])\n",
    "data=np.concatenate((data,data3))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d6c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136989\n"
     ]
    }
   ],
   "source": [
    "# Assigning '4' to column 4\n",
    "data4=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/4')\n",
    "\n",
    "for i in range(0,len(data4)):\n",
    "    data4[i]=np.append(data4[i],['4'])\n",
    "data=np.concatenate((data,data4))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b8e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140534\n"
     ]
    }
   ],
   "source": [
    "# Assigning '5' to column 5\n",
    "data5=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/5')\n",
    "\n",
    "for i in range(0,len(data5)):\n",
    "    data5[i]=np.append(data5[i],['5'])\n",
    "data=np.concatenate((data,data5))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "800bf9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143652\n"
     ]
    }
   ],
   "source": [
    "# Assigning '6' to column 6\n",
    "data6=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/6')\n",
    "\n",
    "for i in range(0,len(data6)):\n",
    "    data6[i]=np.append(data6[i],['6'])\n",
    "data=np.concatenate((data,data6))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcfb3e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146561\n"
     ]
    }
   ],
   "source": [
    "# Assigning '7' to column 7\n",
    "data7=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/7')\n",
    "\n",
    "for i in range(0,len(data7)):\n",
    "    data7[i]=np.append(data7[i],['7'])\n",
    "data=np.concatenate((data,data7))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53c810ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149629\n"
     ]
    }
   ],
   "source": [
    "# Assigning '8' to column 8\n",
    "data8=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/8')\n",
    "\n",
    "for i in range(0,len(data8)):\n",
    "    data8[i]=np.append(data8[i],['8'])\n",
    "data=np.concatenate((data,data8))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a0355e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153366\n"
     ]
    }
   ],
   "source": [
    "# Assigning '9' to column 9\n",
    "data9=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/9')\n",
    "\n",
    "for i in range(0,len(data9)):\n",
    "    data9[i]=np.append(data9[i],['9'])\n",
    "data=np.concatenate((data,data9))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d0ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156617\n"
     ]
    }
   ],
   "source": [
    "# Assigning '* or times' to column 12\n",
    "data12=load_images_from_folder('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/extracted_images/times')\n",
    "\n",
    "for i in range(0,len(data12)):\n",
    "    data12[i]=np.append(data12[i],['12'])\n",
    "data=np.concatenate((data,data12))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d29f7d",
   "metadata": {},
   "source": [
    "## Dataframe creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae60cfd",
   "metadata": {},
   "source": [
    "We will now create a seperate dataframe using the 'data' list so that we can use that dataframe in the subsequent model building process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "780411d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, index=None)\n",
    "df.to_csv('train_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4272f1",
   "metadata": {},
   "source": [
    "As we can see, a new dataframe called 'train_final' is created and is filled with all the image sets as records in various columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c7090",
   "metadata": {},
   "source": [
    "## Creating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec44a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_final.csv', index_col = False)\n",
    "labels = df_train[['784']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb586b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  774  775  776  777  \\\n",
       "0  255  255  255  255  255  255  255  255  255  255  ...    0    0    0    0   \n",
       "1  255  255  255  255  255  255  255  255  255  255  ...    0    0    0    0   \n",
       "2  255  255  255  255  255  255  255  255  255  255  ...    0    0    0    0   \n",
       "3  255  255  255  255  255  255  255  255  255  255  ...    0    0    0    0   \n",
       "4  255  255  255  255  255  255  255  255  255  255  ...    0    0    0    0   \n",
       "\n",
       "   778  779  780  781  782  783  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.drop(df_train.columns[[784]], axis=1, inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501144fd",
   "metadata": {},
   "source": [
    "We need the labels of the dataset in a seperate variable which we will use in our category variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6abb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430526b9",
   "metadata": {},
   "source": [
    "Using the labels we will create our category variable which will be used while fitting the model to the dataset. This is done so that we can fit our model more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67ff439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = to_categorical(labels, num_classes=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "617fd1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34146fa5",
   "metadata": {},
   "source": [
    "Next we will be creating a list with the 'lst' variable and appending all the training set values to it.\n",
    "We will also reshape the lst in the format of (28, 28, 1) because this is how we will be feeding it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "571f052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(156617):\n",
    "    lst.append(np.array(df_train[i:i+1]).reshape(28,28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63976f7b",
   "metadata": {},
   "source": [
    "Resetting the seed value to 7 which we had changed in the beginning of our project code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6acf62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0830a",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193136e1",
   "metadata": {},
   "source": [
    "The CNN model will be defined as per our dataset. The layers to be added in the model are:\n",
    "- __Convolutional Layer__: This layer is the first layer that is used to extract the various features from the input images. We will be using __'_Conv2D_'__ for our convolutional layer requirements\n",
    "\n",
    "\n",
    "- __Fully Connected Layer__: The Fully Connected (FC) layer consists of the weights and biases along with the neurons and is used to connect the neurons between two different layers. __'_Dense_'__ will be utilized for creating a FC layer here.\n",
    "\n",
    "\n",
    "- __Dropout__: Usually, when all the features are connected to the FC layer, it can cause overfitting in the training dataset. Overfitting occurs when a particular model works so well on the training data causing a negative impact in the model’s performance when used on a new data. \n",
    "    To overcome this problem, a ___dropout layer___ is utilised wherein a few neurons are dropped from the neural network during training process resulting in reduced size of the model. \n",
    "    \n",
    "\n",
    "- __Activation Functions__: Finally, one of the most important parameters of the CNN model is the activation function. They are used to learn and approximate any kind of continuous and complex relationship between variables of the network. In simple words, it decides which information of the model should fire in the forward direction and which ones should not at the end of the network.\n",
    "    __'_Softmax activation_'__ function will be used which will convert all predictions into probability. Tuning can be performed on the architecture further to get the most optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce286608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02f70c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, 24, 24, 30)        780       \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 12, 12, 30)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 10, 10, 15)        4065      \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 5, 5, 15)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 5, 5, 15)          0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 375)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               48128     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 50)                6450      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 13)                663       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,086\n",
      "Trainable params: 60,086\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b229a",
   "metadata": {},
   "source": [
    "__Note__: The shape of input required is of (28, 28, 1) type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff222b",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c81c20",
   "metadata": {},
   "source": [
    "We will be making use of the category variable created earlier for the model fitting process.\n",
    "The input will be lst in the form of array with batch size of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "346e1bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "784/784 [==============================] - 122s 148ms/step - loss: 0.4060 - accuracy: 0.8960\n",
      "Epoch 2/10\n",
      "784/784 [==============================] - 140s 178ms/step - loss: 0.0961 - accuracy: 0.9720\n",
      "Epoch 3/10\n",
      "784/784 [==============================] - 114s 145ms/step - loss: 0.0664 - accuracy: 0.9804\n",
      "Epoch 4/10\n",
      "784/784 [==============================] - 101s 129ms/step - loss: 0.0513 - accuracy: 0.9852\n",
      "Epoch 5/10\n",
      "784/784 [==============================] - 117s 149ms/step - loss: 0.0419 - accuracy: 0.9874\n",
      "Epoch 6/10\n",
      "784/784 [==============================] - 128s 163ms/step - loss: 0.0373 - accuracy: 0.9889\n",
      "Epoch 7/10\n",
      "784/784 [==============================] - 130s 166ms/step - loss: 0.0299 - accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "784/784 [==============================] - 125s 160ms/step - loss: 0.0296 - accuracy: 0.9911\n",
      "Epoch 9/10\n",
      "784/784 [==============================] - 122s 155ms/step - loss: 0.0245 - accuracy: 0.9928\n",
      "Epoch 10/10\n",
      "784/784 [==============================] - 136s 174ms/step - loss: 0.0251 - accuracy: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2268d9e6980>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(lst), cat, epochs=10, batch_size=200,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b387b7d",
   "metadata": {},
   "source": [
    "Since the model has been trained successfully, lets save this model before we move onto any other evaluation or prediction stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "145521f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"LGMVIP_Task9_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"LGMVIP_Task9_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3c203",
   "metadata": {},
   "source": [
    "## Making Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81717915",
   "metadata": {},
   "source": [
    "Since our model has been created, we will now move onto the predictions. The initial procedure is almost same as we did while loading our dataset. The steps to be followed here are:\n",
    "\n",
    "- Input an image containing a handwritten equation. Convert the image to a binary image and then invert the image(if digits/symbols are in black).\n",
    "\n",
    "- Obtaining the contours of the image by default, it will obtain contours from left to right.\n",
    "\n",
    "- Obtaining bounding rectangle for each contour.\n",
    "\n",
    "- Sometimes, we may get two or more contours for the same digit/symbol. To avoid that, we can check if the bounding rectangle of those two contours overlaps or not. If they overlap, then discard the smaller rectangle.\n",
    "\n",
    "- Now, resize all the remaining bounding rectangle to 28 by 28.\n",
    "\n",
    "Lastly, we will use our model to predict the corresponding digit/symbol for each bounding rectangle and store it in a string.\n",
    "After that evaluate the string to produce our results and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07eb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('E:/Data-Science-Projects/LGMVIP--DataScience/Task -9/test.jpeg',cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow(\"wo\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "if img is not None:\n",
    "    img=~img\n",
    "    ret,thresh=cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "    ctrs,hie=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt=sorted(ctrs, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "    w=int(28)\n",
    "    h=int(28)\n",
    "    train_data=[]\n",
    "    rects=[]\n",
    "    for c in cnt :\n",
    "        x,y,w,h= cv2.boundingRect(c)\n",
    "        rect=[x,y,w,h]\n",
    "        rects.append(rect)\n",
    "    bool_rect=[]\n",
    "    for r in rects:\n",
    "        l=[]\n",
    "        for rec in rects:\n",
    "            flag=0\n",
    "            if rec!=r:\n",
    "                if r[0]<(rec[0]+rec[2]+10) and rec[0]<(r[0]+r[2]+10) and r[1]<(rec[1]+rec[3]+10) and rec[1]<(r[1]+r[3]+10):\n",
    "                    flag=1\n",
    "                l.append(flag)\n",
    "            if rec==r:\n",
    "                l.append(0)\n",
    "        bool_rect.append(l)\n",
    "    dump_rect=[]\n",
    "    for i in range(0,len(cnt)):\n",
    "        for j in range(0,len(cnt)):\n",
    "            if bool_rect[i][j]==1:\n",
    "                area1=rects[i][2]*rects[i][3]\n",
    "                area2=rects[j][2]*rects[j][3]\n",
    "                if(area1==min(area1,area2)):\n",
    "                    dump_rect.append(rects[i])\n",
    "    final_rect=[i for i in rects if i not in dump_rect]\n",
    "    for r in final_rect:\n",
    "        x=r[0]\n",
    "        y=r[1]\n",
    "        w=r[2]\n",
    "        h=r[3]\n",
    "        im_crop =thresh[y:y+h+10,x:x+w+10]\n",
    "        \n",
    "\n",
    "        im_resize = cv2.resize(im_crop,(28,28))\n",
    "        cv2.imshow(\"work\",im_resize)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        im_resize=np.reshape(im_resize,(1,28,28))\n",
    "        train_data.append(im_resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f5536",
   "metadata": {},
   "source": [
    "The processing of the test input image has been completed and we have our final 'train_data' ready for evaluation. Here we will use our model to analyze the input data and make prediction accordingly to check the accuracy of our model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e030f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=''\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i]=np.array(train_data[i])\n",
    "    train_data[i]=train_data[i].reshape(1,1,28,28)\n",
    "    result=model.predict_classes(train_data[i])\n",
    "    if(result[0]==10):\n",
    "        s=s+'-'\n",
    "    if(result[0]==11):\n",
    "        s=s+'+'\n",
    "    if(result[0]==12):\n",
    "        s=s+'*'\n",
    "    if(result[0]==0):\n",
    "        s=s+'0'\n",
    "    if(result[0]==1):\n",
    "        s=s+'1'\n",
    "    if(result[0]==2):\n",
    "        s=s+'2'\n",
    "    if(result[0]==3):\n",
    "        s=s+'3'\n",
    "    if(result[0]==4):\n",
    "        s=s+'4'\n",
    "    if(result[0]==5):\n",
    "        s=s+'5'\n",
    "    if(result[0]==6):\n",
    "        s=s+'6'\n",
    "    if(result[0]==7):\n",
    "        s=s+'7'\n",
    "    if(result[0]==8):\n",
    "        s=s+'8'\n",
    "    if(result[0]==9):\n",
    "        s=s+'9'\n",
    "    \n",
    "print(s) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e907a8",
   "metadata": {},
   "source": [
    "The project has been completed successfully and the model has been trained to be reliable and efficient enough the make predictions in realtime scenarios on unseen datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd7082",
   "metadata": {},
   "source": [
    "__Note__: The model requires Tensorflow to work with CUDA which is only supported by NVIDIA GPUs. Since the system on which this project was developed had AMD powered GPU, making predictions was not a possibility but the model is reliable enough to be tested elsewhere in systems having all necessary requirements to support the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
